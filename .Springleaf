import pandas as pd
from collections import defaultdict 
import numpy as np
from sklearn.svm import SVC
from sklearn.decomposition import TruncatedSVD
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
from sklearn import ensemble, preprocessing, grid_search, cross_validation
from scipy import interp
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.metrics import roc_auc_score
from sklearn.cross_validation import StratifiedKFold
from sklearn import linear_model
"""
Fitting 3 folds for each of 8 candidates, totalling 24 fits
[ 0.79141104  0.79060544  0.79200149]
mean 0.791339323728
Fitting 3 folds for each of 8 candidates, totalling 24 fits[Parallel(n_jobs=2)]: Done   1 jobs       | elapsed: 10.7min

[Parallel(n_jobs=2)]: Done  24 out of  24 | elapsed: 206.7min finished
>>> >>> classifier.best_estimator_
GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',
              max_depth=6, max_features=40, max_leaf_nodes=None,
              min_samples_leaf=1, min_samples_split=25, n_estimators=200,
              random_state=42, subsample=1.0, verbose=0, warm_start=False)
"""

if __name__ == '__main__':
    train_cat = pd.read_csv('D:/springleaf/videoon ensemble/st_train2.csv')#, nrows = 10000) 
    test_cat = pd.read_csv('D:/springleaf/videoon ensemble/st_test2.csv')#,  nrows = 10000)

    train_staked = pd.read_csv('D:/springleaf/videoon ensemble/stakedop/train_prob_5fold.csv')#, nrows = 10000) 
    test_staked = pd.read_csv('D:/springleaf/videoon ensemble/stakedop/test_prob_5fold.csv')#,  nrows = 10000)
    train_id = train_cat.ID.values
    target = train_cat.target.values
    test_id = test_cat.ID
    print train_cat.shape , test_cat.shape, train_staked.shape, test_staked.shape
    
       
    
    columns_need_to_be_drop = ['VAR_0001','VAR_0005', 'VAR_0073', 'VAR_0075', 'VAR_0156', 'VAR_0157',
       'VAR_0158', 'VAR_0159', 'VAR_0166', 'VAR_0167', 'VAR_0168',
       'VAR_0169', 'VAR_0176', 'VAR_0177', 'VAR_0178', 'VAR_0179',
       'VAR_0200', 'VAR_0204', 'VAR_0217', 'VAR_0237', 'VAR_0274', 'VAR_0283',
       'VAR_0305', 'VAR_0325', 'VAR_0342', 'VAR_0352', 'VAR_0353',
       'VAR_0354', 'VAR_0404', 'VAR_0466', 'VAR_0467', 'VAR_0493',
       'VAR_1934']
       
    train_cat = train_cat.drop(columns_need_to_be_drop,axis =1) 
    test_cat = test_cat.drop(columns_need_to_be_drop,axis =1)
    train_cat = train_cat.drop(['target'],axis =1) 
    train_staked = train_staked.drop(['target'],axis =1)  
     
    train = train_cat.merge(train_staked , on = 'ID')
    test = test_cat.merge(test_staked , on = 'ID')
    
    
    #train_cat = train_cat[['VAR_0226', 'VAR_0230', 'VAR_0232', 'VAR_0236', 'VAR_0294', 'VAR_0332', 'VAR_0005inteaction1', 'VAR_0005inteaction0', 'VAR_1934inteaction1', 'VAR_1934inteaction0', 'VAR_0237inteaction1', 'VAR_0075m', 'VAR_0166w']]
    #test_cat = test_cat[['VAR_0226', 'VAR_0230', 'VAR_0232', 'VAR_0236', 'VAR_0294', 'VAR_0332', 'VAR_0005inteaction1', 'VAR_0005inteaction0', 'VAR_1934inteaction1', 'VAR_1934inteaction0', 'VAR_0237inteaction1', 'VAR_0075m', 'VAR_0166w']]
    
        
    
    
    
        
    
    train = train.replace(np.nan,-1)
    test = test.replace(np.nan,-1)
    print train.shape , test.shape
    
    
    train = train.drop(['ID'],axis = 1)
    test = test.drop(['ID'],axis = 1)    
    
    k = train.columns    
    print train.shape,test.shape
    train =np.array(train)
    test = np.array(test)
    train = train.astype(float)
    test = test.astype(float)  
    
    
    
    

    
    
    
    
    
    np.random.seed(0) # seed to shuffle the train set

    n_folds = 10
    X = train
    train = 1
    y = target
    X_submission = test  
    
    test = 1
    
    print 'started stacking...........'
    skf = list(StratifiedKFold(y, n_folds))

    clfs = [GradientBoostingClassifier(random_state = 42,n_estimators=250,min_samples_split= 25,max_depth = 2,learning_rate = 0.1, max_features=10, loss='deviance'),
            GradientBoostingClassifier(random_state = 42,n_estimators=250,min_samples_split= 25,max_depth = 3,learning_rate = 0.1, max_features=20, loss='deviance'),
            GradientBoostingClassifier(random_state = 42,n_estimators=250,min_samples_split= 25,max_depth = 4,learning_rate = 0.1, max_features=24, loss='deviance'),
            GradientBoostingClassifier(random_state = 42,n_estimators=250,min_samples_split= 25,max_depth = 2,learning_rate = 0.1, max_features=6, loss='deviance')]

    print "Creating train and test sets for blending."
    
    dataset_blend_train = np.zeros((X.shape[0], len(clfs)))
    dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))
    
    for j, clf in enumerate(clfs):
        print j, clf
        dataset_blend_test_j = np.zeros((X_submission.shape[0], len(skf)))
        for i, (train, test) in enumerate(skf):
            print "Fold", i
            X_train = X[train]
            y_train = y[train]
            X_test = X[test]
            y_test = y[test]
            clf.fit(X_train, y_train)
            y_submission = clf.predict_proba(X_test)[:,1]
            dataset_blend_train[test, j] = y_submission
            dataset_blend_test_j[:, i] = clf.predict_proba(X_submission)[:,1]
        dataset_blend_test[:,j] = dataset_blend_test_j.mean(1)
        pd.DataFrame(dataset_blend_test).to_csv(str(j)+'test5fold.csv' , index = False)
        pd.DataFrame(dataset_blend_train).to_csv(str(j)+'train5fold.csv' , index = False)
    print
    print "Blending."
    params = [{'penalty':['l1','l2'], 'C':[0.1,0.4,0.7,1,2,3,4,5,6,7,8,9,10], 'fit_intercept': ['True','False'] }]       
    
    
    
    
    classifier = grid_search.GridSearchCV(clf,params, verbose=1, n_jobs = 2,cv = 10)
    print "uhsgsggggsgsgsgggg"
    
    
    score = cross_validation.cross_val_score(classifier,dataset_blend_train,y,scoring = 'roc_auc')
    
    
    print score
    
    print 'mean', np.mean(score)
    
    classifier.fit(dataset_blend_train, y)
    y_submission = classifier.predict_proba(dataset_blend_test)[:,1]

    """
    print "Linear stretch of predictions to [0,1]"
    y_submission1 = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())
    """
    print "Saving Results."
       
    prob = pd.DataFrame({"ID": test_id, "target": y_submission})
    prob.to_csv('stacked_result.csv', index=False) 
    
    
    train1 = pd.DataFrame({"ID":train_id,"s_1": dataset_blend_train[:,0],"s_2": dataset_blend_train[:,1], "s_3": dataset_blend_train[:,2], "s_4": dataset_blend_train[:,3], "target": target}) 

    test1 = pd.DataFrame({"ID":test_id,"s_1": dataset_blend_test[:,0],"s_2": dataset_blend_test[:,1], "s_3": dataset_blend_test[:,2], "s_4": dataset_blend_test[:,3]}) 
    
    train1.to_csv('train_prob_10fold.csv', index = False)
    test1.to_csv('test_prob_10fold.csv', index = False)
